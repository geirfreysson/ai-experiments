[
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "About this blog"
  },
  {
    "objectID": "posts/15-jan-2024-titanic/index.html",
    "href": "posts/15-jan-2024-titanic/index.html",
    "title": "Predicting outcomes for passengers on the Titanic with TensorFlow",
    "section": "",
    "text": "The passenger list on the Titanic is a popular dataset for machine learning, so I thought it was a fitting way to start this documentation of my AI experiements.\nIt’s a slightly morbid dataset made classic by the fact that it’s the first dataset used for anyone starting out on Kaggle, the data science competition platform.\nThe idea is to use data science to predict whether a passenger will survive or not, using a information we have about the passengers."
  },
  {
    "objectID": "posts/15-jan-2024-titanic/index.html#import-and-explore",
    "href": "posts/15-jan-2024-titanic/index.html#import-and-explore",
    "title": "Predicting outcomes for passengers on the Titanic with TensorFlow",
    "section": "Import and explore",
    "text": "Import and explore\nFirst, we import pandas and explore the data.\n\nimport pandas as pd\nimport tensorflow as tf\nfrom sklearn.preprocessing import MinMaxScaler\nfrom sklearn.model_selection import train_test_split\n\ndata = pd.read_csv('titanic.csv')\ndata.head()\n\n\n  \n    \n\n\n\n\n\n\nPassengerId\nSurvived\nPclass\nName\nSex\nAge\nSibSp\nParch\nTicket\nFare\nCabin\nEmbarked\n\n\n\n\n0\n1\n0\n3\nBraund, Mr. Owen Harris\nmale\n22.0\n1\n0\nA/5 21171\n7.2500\nNaN\nS\n\n\n1\n2\n1\n1\nCumings, Mrs. John Bradley (Florence Briggs Th...\nfemale\n38.0\n1\n0\nPC 17599\n71.2833\nC85\nC\n\n\n2\n3\n1\n3\nHeikkinen, Miss. Laina\nfemale\n26.0\n0\n0\nSTON/O2. 3101282\n7.9250\nNaN\nS\n\n\n3\n4\n1\n1\nFutrelle, Mrs. Jacques Heath (Lily May Peel)\nfemale\n35.0\n1\n0\n113803\n53.1000\nC123\nS\n\n\n4\n5\n0\n3\nAllen, Mr. William Henry\nmale\n35.0\n0\n0\n373450\n8.0500\nNaN\nS\n\n\n\n\n\n    \n\n  \n    \n\n  \n    \n  \n    \n\n  \n\n    \n  \n\n\n\n  \n\n\n    \n        \n    \n\n  \n\n\n\n  \n\n    \n  \n\n\nThe passenger list includes different bits of information about the passengers, such as their age, gender, what class they travelled on, how much they paid for their ticket, and so on. For more information about the data, see the Kaggle competition webpage.\nIf we look at the first two rows in the data, we see Mr. Owen Harris Braund and Mrs. John Bradley (Florence Briggs Thayer).\nMrs. Bradley paid ten times what Mr. Owen did for her fair, travelled on first class, while he travelled third class, and she also survived, unlike Mr. Owen.\n\n\n\n\n\n\n\n\n\nMr. Owen\n\n\n\n\n\n\n\nMrs. Bradley\n\n\n\n\n\n\ndata.iloc[0:2].set_index('Name').T\n\n\n  \n    \n\n\n\n\n\nName\nBraund, Mr. Owen Harris\nCumings, Mrs. John Bradley (Florence Briggs Thayer)\n\n\n\n\nPassengerId\n1\n2\n\n\nSurvived\n0\n1\n\n\nPclass\n3\n1\n\n\nSex\nmale\nfemale\n\n\nAge\n22.0\n38.0\n\n\nSibSp\n1\n1\n\n\nParch\n0\n0\n\n\nTicket\nA/5 21171\nPC 17599\n\n\nFare\n7.25\n71.2833\n\n\nCabin\nNaN\nC85\n\n\nEmbarked\nS\nC"
  },
  {
    "objectID": "posts/15-jan-2024-titanic/index.html#select-and-prepare-data",
    "href": "posts/15-jan-2024-titanic/index.html#select-and-prepare-data",
    "title": "Predicting outcomes for passengers on the Titanic with TensorFlow",
    "section": "Select and prepare data",
    "text": "Select and prepare data\nFor the purpose of this excersize, we are only going to use data about survival, passenger class, age, siblings on board and parents on board. We will ignore the fare, cabin number and what port they sailed from.\n\ndata = data[['Survived', 'Pclass', 'Sex', 'Age', 'SibSp', 'Parch']]\n\n\n# Impute missing values with the median\ndata['Age'] = data['Age'].fillna(data['Age'].median())\n# scale/normalize age\nscaler = MinMaxScaler()\ndata['Age'] = scaler.fit_transform(data[['Age']])\n\n# one-hot encode sex\ndata = pd.get_dummies(data, columns=['Sex'])\n\nAfter a bit of data wrangling, we’ve changed the data we are working with to a more machine friendly verison, by normalizing the age (making it into a scale of 0-1 with the oldest person on board being 1) and “one-hot encoding” their gender.\n\ndata.head()\n\n\n  \n    \n\n\n\n\n\n\nSurvived\nPclass\nAge\nSibSp\nParch\nSex_female\nSex_male\n\n\n\n\n0\n0\n3\n0.271174\n1\n0\n0\n1\n\n\n1\n1\n1\n0.472229\n1\n0\n1\n0\n\n\n2\n1\n3\n0.321438\n0\n0\n1\n0\n\n\n3\n1\n1\n0.434531\n1\n0\n1\n0\n\n\n4\n0\n3\n0.434531\n0\n0\n0\n1"
  },
  {
    "objectID": "posts/15-jan-2024-titanic/index.html#split-the-dataset-into-training-and-testing",
    "href": "posts/15-jan-2024-titanic/index.html#split-the-dataset-into-training-and-testing",
    "title": "Predicting outcomes for passengers on the Titanic with TensorFlow",
    "section": "Split the dataset into training and testing",
    "text": "Split the dataset into training and testing\nWe need to split the data into the batch we will use to train the neural network model and a batch that the model has never seen, to test how good it is.\n\n# Split the dataset into train and test sets\ntrain_df, test_df = train_test_split(data, test_size=0.2, random_state=42)\n\n# Further split the training set into train and validation sets\ntrain_df, val_df = train_test_split(train_df, test_size=0.25, random_state=42) # 0.25 x 0.8 = 0.2\n\n\nX_train = train_df.drop(\"Survived\", axis=1)\nY_train = train_df['Survived']\n\nX_test = test_df.drop(\"Survived\", axis=1)\nY_test = test_df['Survived']\nprint(X_train.shape)\nprint(Y_train.shape)\n\n(534, 6)\n(534,)"
  },
  {
    "objectID": "posts/15-jan-2024-titanic/index.html#create-a-deep-learning-model-and-train-it",
    "href": "posts/15-jan-2024-titanic/index.html#create-a-deep-learning-model-and-train-it",
    "title": "Predicting outcomes for passengers on the Titanic with TensorFlow",
    "section": "Create a deep learning model and train it",
    "text": "Create a deep learning model and train it\nCreating a basic deep learning model with keras is quite easy, we just create each layer of the model and add them in sequence. We choose dense layers, where each node is connected to every other node in the next layer and have 20 nodes in each layer.\n\nfrom keras.layers import Dense, Input #, Dropout\nfrom keras.models import Sequential\n\nmodel = Sequential()\n\nmodel.add(Dense(units=100, input_shape=(6,), activation='relu'))\nmodel.add(Dense(units=100, activation='relu'))\nmodel.add(Dense(units =1 , activation = 'sigmoid'))\n\nmodel.compile(\n    loss = tf.keras.losses.binary_crossentropy,\n    optimizer = tf.keras.optimizers.Adam(),\n    metrics = ['acc']\n)\nmodel.fit(X_train, Y_train, verbose = 2, epochs = 20)\n\n\nEpoch 1/20\n17/17 - 1s - loss: 0.6533 - acc: 0.5749 - 792ms/epoch - 47ms/step\nEpoch 2/20\n17/17 - 0s - loss: 0.5574 - acc: 0.7360 - 33ms/epoch - 2ms/step\nEpoch 3/20\n17/17 - 0s - loss: 0.5080 - acc: 0.7884 - 34ms/epoch - 2ms/step\nEpoch 4/20\n17/17 - 0s - loss: 0.4818 - acc: 0.7921 - 30ms/epoch - 2ms/step\nEpoch 5/20\n17/17 - 0s - loss: 0.4713 - acc: 0.7959 - 35ms/epoch - 2ms/step\nEpoch 6/20\n17/17 - 0s - loss: 0.4676 - acc: 0.7884 - 32ms/epoch - 2ms/step\nEpoch 7/20\n17/17 - 0s - loss: 0.4585 - acc: 0.8052 - 35ms/epoch - 2ms/step\nEpoch 8/20\n17/17 - 0s - loss: 0.4515 - acc: 0.7996 - 33ms/epoch - 2ms/step\nEpoch 9/20\n17/17 - 0s - loss: 0.4430 - acc: 0.8090 - 34ms/epoch - 2ms/step\nEpoch 10/20\n17/17 - 0s - loss: 0.4421 - acc: 0.8071 - 33ms/epoch - 2ms/step\nEpoch 11/20\n17/17 - 0s - loss: 0.4354 - acc: 0.8090 - 35ms/epoch - 2ms/step\nEpoch 12/20\n17/17 - 0s - loss: 0.4357 - acc: 0.8071 - 36ms/epoch - 2ms/step\nEpoch 13/20\n17/17 - 0s - loss: 0.4325 - acc: 0.8109 - 33ms/epoch - 2ms/step\nEpoch 14/20\n17/17 - 0s - loss: 0.4328 - acc: 0.7959 - 35ms/epoch - 2ms/step\nEpoch 15/20\n17/17 - 0s - loss: 0.4399 - acc: 0.7959 - 32ms/epoch - 2ms/step\nEpoch 16/20\n17/17 - 0s - loss: 0.4255 - acc: 0.8034 - 35ms/epoch - 2ms/step\nEpoch 17/20\n17/17 - 0s - loss: 0.4220 - acc: 0.8127 - 35ms/epoch - 2ms/step\nEpoch 18/20\n17/17 - 0s - loss: 0.4217 - acc: 0.8109 - 32ms/epoch - 2ms/step\nEpoch 19/20\n17/17 - 0s - loss: 0.4212 - acc: 0.8090 - 35ms/epoch - 2ms/step\nEpoch 20/20\n17/17 - 0s - loss: 0.4172 - acc: 0.8184 - 32ms/epoch - 2ms/step\n\n\n&lt;keras.src.callbacks.History at 0x7e0516b54f40&gt;"
  },
  {
    "objectID": "posts/15-jan-2024-titanic/index.html#test-the-model-on-data-it-hasnt-seen",
    "href": "posts/15-jan-2024-titanic/index.html#test-the-model-on-data-it-hasnt-seen",
    "title": "Predicting outcomes for passengers on the Titanic with TensorFlow",
    "section": "Test the model on data it hasn’t seen",
    "text": "Test the model on data it hasn’t seen\nNow that we’ve successfully trained the model and the loss function has stopped improving\n\nloss, accuracy = model.evaluate(X_test, Y_test)\nprint(f\"Test Loss: {loss}\")\nprint(f\"Test Accuracy: {accuracy}\")\n\n6/6 [==============================] - 0s 3ms/step - loss: 0.4432 - acc: 0.8156\nTest Loss: 0.44318273663520813\nTest Accuracy: 0.8156424760818481\n\n\nThe accuracy is higher than 0.8, which is a very good score on this particular challenge.\nIn Titanic leaderboard: a score &gt; 0.8 is great!, Carl Ellis analyses how people are scoring in the competition, and the image shows that any score above 0.8 is ahead of the curve.\n\n\n\nScore distribution from Kaggle competition"
  },
  {
    "objectID": "posts/15-jan-2024-titanic/index.html#results",
    "href": "posts/15-jan-2024-titanic/index.html#results",
    "title": "Predicting outcomes for passengers on the Titanic with TensorFlow",
    "section": "Results",
    "text": "Results\nIt’s quite amazing how we can get such good results with a very basic deep learning model. There’s a lot that can be done to add finesse to the model, but the out-of-the box standard one gets very good results. The model has just over 10k parameters, which is not much compared to the billions of parameters used in large language models, so training it takes less than a second."
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Geir's notes",
    "section": "",
    "text": "Jan 8, 2025\n                            Building one-shot applications with uv and Claude\n                            \n                                \n                                \n                                    The author has been using an AI model called Claude to build one-shot applications, including HTML+JavaScript tools and Python utilities. By setting up custom instructions in a Claude Project, he can have Claude write full-featured scripts that include dependencies without needing extra installation steps. Using uv run, a tool for running inline script dependencies, the author can create and run these one-shot scripts directly from URLs or prompts, making it easy to build tools like a debug script for Amazon S3 buckets or a web server with an API.\n                                \n                            \n                            Read More\n                    \n                        \n                        \n\n                        \n                            Jan 4, 2025\n                            Summary of https://minimaxir.com/2025/01/write-better-code/\n                            \n                                \n                                \n                                    \n                                \n                            \n                            Read More\n                    \n                        \n                        \n\n                        \n                            Jan 21, 2024\n                            Predicting outcomes for passengers on the Titanic with TensorFlow\n                            \n                                \n                                \n                                    \n                                \n                            \n                            Read More\n                    \n                        \n                        \n\n                    \n                    No matching items"
  },
  {
    "objectID": "posts/12-jan-2025-auto-link-blog/index.html",
    "href": "posts/12-jan-2025-auto-link-blog/index.html",
    "title": "Building one-shot applications with uv and Claude",
    "section": "",
    "text": "Summary of https://simonwillison.net/2024/Dec/19/one-shot-python-tools/\n\n\nThe following is an outline of the website content.\nHere is a short paragraph summarizing the content:\nThe author has been using an AI model called Claude to build one-shot applications, including HTML+JavaScript tools and Python utilities. By setting up custom instructions in a Claude Project, he can have Claude write full-featured scripts that include dependencies without needing extra installation steps. Using uv run, a tool for running inline script dependencies, the author can create and run these one-shot scripts directly from URLs or prompts, making it easy to build tools like a debug script for Amazon S3 buckets or a web server with an API."
  },
  {
    "objectID": "posts/31-12-2024-auto-summarize-for-blog/summary_2025-01-04.html",
    "href": "posts/31-12-2024-auto-summarize-for-blog/summary_2025-01-04.html",
    "title": "Summary of https://minimaxir.com/2025/01/write-better-code/",
    "section": "",
    "text": "Summary of https://minimaxir.com/2025/01/write-better-code/\n\n\nThe following is an outline of the website content.\nWhen generating distinct AI voice performances, the use of prompt engineering techniques can significantly enhance the versatility and performance of GPT-4. These techniques allow for fine-tuning the AI model to achieve specific vocal characteristics or expressions based on user input. By employing a range of prompts that cater to diverse vocal demands, such as pitch, intonation, and accent, we can elicit more dynamic and versatile output from GPT-4.\nTo begin with, consider providing the AI model with precise instructions regarding the desired vocal characteristics. This may include directives on the target language or accent, as well as guidelines for specific emotional expressions or vocal tones. For example, if you want a robotic voice, you can prompt the AI model to mimic a monotonous tone with minimal pitch variation.\nAnother effective approach is to utilize text-to-speech (TTS) engines that allow users to input phonetic transcriptions of words or phrases, alongside their desired pronunciation. By incorporating these TTS features into the AI model’s prompt engineering framework, you can achieve a higher level of control over the output vocal performance.\nIt is also essential to provide contextual information, such as cultural references or situational cues, that may impact the vocal characteristics and expressions. For instance, when generating an American accent, it would be beneficial to include colloquialisms, slang terms, or regional speech patterns. Similarly, for a foreign language, incorporating idioms, proverbs, or idiomatic expressions will contribute to the authenticity of the AI-generated voice.\nTo ensure that GPT-4’s output aligns with your preferences, you can train it on large datasets of audio recordings featuring various vocal performances. By exposing the model to a diverse array of spoken samples, it can learn to generate more nuanced and distinct voices tailored to specific user requirements.\nIn addition to training the AI model with diverse voice samples, incorporating feedback loops within the prompt engineering framework allows for continuous improvement of the generated output. This enables users to refine their prompts based on feedback from the initial outputs, leading to a more polished and personalized vocal performance over time.\nTo achieve a higher level of accuracy and authenticity in AI-generated voices, consider incorporating deep learning techniques such as transfer learning or pretrained models. These approaches allow the model to leverage existing knowledge about voice generation while fine-tuning it to specific user requirements. By combining prompt engineering with advanced machine learning algorithms, we can create more lifelike and expressive AI voice performances.\nOverall, by leveraging the power of prompt engineering and incorporating a range of vocal characteristics and expressions, GPT-4 can deliver distinct and versatile AI voice performances that cater to diverse user demands. This opens up new possibilities for various applications, including virtual assistants, gaming experiences, and multimedia productions."
  }
]