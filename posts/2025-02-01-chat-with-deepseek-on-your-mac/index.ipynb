{
 "cells": [
  {
   "cell_type": "raw",
   "id": "74f249ad",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "---\n",
    "title: \"Chat with Deepseek on your Mac via the browser\"\n",
    "date: \"2025-02-01\"\n",
    "draft: false\n",
    "publish: true\n",
    "callout-appearance: simple\n",
    "categories: [llm, webui, ollama, deepseek]\n",
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91f86905",
   "metadata": {},
   "source": [
    "LLM models that run on your local machine are becoming better and better. Indeed, thanks to the AI arms race, these models will soon be good enough for many tasks required by individuals and companies looking to leverage AI. I've [already covered](/posts/2025-01-19-playing-with-embeddings/index.html#using-local-llms) how to install an LLM locally using Ollama, but how do you chat with the model in a browser? Enter WebUI.\n",
    "\n",
    "## Installing WebUI\n",
    "[WebUI](https://openwebui.com/) is an open source project that describes itself as\n",
    "\n",
    "> an extensible, self-hosted AI interface that adapts to your workflow, all while operating entirely offline\n",
    "\n",
    "Their first recommended installation option, using Docker, didn't work for me, but luckily they provide a guide on how to install it via the new python package manager uv.\n",
    "\n",
    "Getting started is as simple as opening your terminal and pasting this in:\n",
    "\n",
    "```{.terminal}\n",
    "DATA_DIR=~/.open-webui uvx --python 3.11 open-webui@latest serve\n",
    "```\n",
    "\n",
    ":::{.callout-note}\n",
    "The easiest way to chat with a local model is probably [LM Studio](https://lmstudio.ai/). It provides a GUI that allows users to download models via a point-and-click interface.. I'm more interested in an option that can become part of a product and is more customisable.\n",
    ":::\n",
    "\n",
    "### It's alive - launching WebUI\n",
    "After waiting for everything to download, the server is running.\n",
    "\n",
    "![](webui-screenshot.png)\n",
    "\n",
    "### Chatting about PDF documents\n",
    "I uploaded four documents from the RAG section of Latent Space's [The 2025 AI Engineer Reading List](https://www.latent.space/p/2025-papers) and asked:\n",
    "\n",
    "```{.terminal}\n",
    "what are the uploaded documents about?\n",
    "```\n",
    "\n",
    "![Screenshot of WebUI running Deepseek anwering questions about my documents.](what-are-the-documents-about.png)\n",
    "\n",
    "The LLM running on my Mac, Deepseek's 8 billion parameter [model](https://ollama.com/library/deepseek-r1), did a fair job of telling me what the uploaded documents are about.\n",
    "\n",
    "Then I asked:\n",
    "```{.terminal}\n",
    "which one of the documents should I look at to know more about benchmarking?\n",
    "```\n",
    "\n",
    "This was less successful. The model just gave me general advice on what to look for, rather than point out the [Massive Text Embedding Benchmark](https://arxiv.org/abs/2210.07316) paper.\n",
    "\n",
    "## Final thoughts\n",
    "Setting up my own (albeit far less capable version) of ChatGPT on my Mac took me 30-60 minutes. WebUI seems to have user management, a plugin architecture and lots more, and it's easy to imagine it running on someone's intranet, connected to documents that can't be shared with the outside world. As the private/local models become more and more powerful, the day every business runs their own internal chat AI is certainly approaching."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99f19264",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
