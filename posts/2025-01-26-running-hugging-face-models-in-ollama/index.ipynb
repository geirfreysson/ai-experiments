{
 "cells": [
  {
   "cell_type": "raw",
   "id": "e21749af",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "---\n",
    "title: Running Hugging Face models in Ollama\n",
    "date: 2025-01-26\n",
    "callout-appearance: simple\n",
    "publish: True\n",
    "draft: False\n",
    "categories: [TIL, ollama, hugging face]\n",
    "image: \"social-media-card.png\"\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b16a8520",
   "metadata": {},
   "source": [
    "I'm a big fan of Ollama, the easiest way to run LLMs on my Mac. Ollama integrates nicely with Hugging Face, the GitHub of LLMs, so it can be very easy to run a model published on Hugging Face with Ollama.\n",
    "\n",
    "I came across a [blog post on Hugging Face](https://huggingface.co/docs/hub/en/ollama) showing how to do this.\n",
    "\n",
    "1. Install Ollama\n",
    "1. Log in to Hugging Face and enable Ollama in the [local apps settings](https://huggingface.co/settings/local-apps)\n",
    "1. Click on the \"use this model\" drop down and click on Ollama\n",
    "\n",
    "That will show you a console command you can paste into your terminal. The model I want to try out is [mxbai-embed-large-v1-Q4_K_M-GGUF](https://huggingface.co/elliotsayes/mxbai-embed-large-v1-Q4_K_M-GGUF) because it was the highest scoring GGUF model I could find on the [embeddings leaderboard](https://huggingface.co/spaces/mteb/leaderboard), and I'm experimenting with embeddings.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8fc92317",
   "metadata": {},
   "source": [
    "```{.terminal .wrap}\n",
    "ollama run hf.co/elliotsayes/mxbai-embed-large-v1-Q4_K_M-GGUF\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2517ab8f",
   "metadata": {},
   "source": [
    "We now have the model accessible to Ollama. Note that the name of the model will be the full path, i.e. `hf.co/.../`.\n",
    "\n",
    "So, to create a vector embedding with the model you would say"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "26914650",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.04531903,\n",
       " -0.015188997,\n",
       " -0.01498892,\n",
       " 0.0064618383,\n",
       " -0.026388915,\n",
       " 0.052302107,\n",
       " -0.033618174,\n",
       " -0.0036809053,\n",
       " 0.026822101,\n",
       " -0.0020189686]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import ollama\n",
    "embeddings = ollama.embed(\n",
    "  model='hf.co/elliotsayes/mxbai-embed-large-v1-Q4_K_M-GGUF',\n",
    "  input='The man with a tie ran for office.',\n",
    ").embeddings\n",
    "embeddings[0][:10]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
