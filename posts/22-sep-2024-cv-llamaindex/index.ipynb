{
  "cells": [
    {
      "cell_type": "raw",
      "metadata": {
        "id": "w-pi8P1Kp9bX",
        "vscode": {
          "languageId": "raw"
        }
      },
      "source": [
        "---\n",
        "title: \"Having a conversation with your documents\"\n",
        "date: \"2024-09-22\"\n",
        "image: titanic.png\n",
        "draft: true\n",
        "callout-appearance: simple\n",
        "categories: [llamaindex, rag, privatellm]\n",
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        ":::{.column-page}\n",
        "![](titanic.png)\n",
        ":::"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5KAH-lEtp9ba"
      },
      "source": [
        "# Conversations with a bunch of resumes\n",
        "\n",
        "If you are growing your team and are gettting lots of applications with resumés and CVs you have to trawl through them in order to find the best candiate.\n",
        "\n",
        "Reading through dozens of resumés is laborious work and you don't want to miss out on the best candidate just because he or she is at the bottom of your pile. This seems like a job for AI (or to be specific, a large language model, an LLM).\n",
        "\n",
        "Our crietria is:\n",
        "\n",
        " - We can't upload the documents to the cloud for privacy reasons\n",
        " - We'd like to be able to find the resumes the LLM likes\n",
        " - We want to have a conversation about all the resumés at once\n",
        "\n",
        "::: {.callout-tip}\n",
        "[Open this notebook in Colab](https://colab.research.google.com/github/geirfreysson/ai-experiments/blob/main/posts/15-jan-2024-titanic/index.ipynb) to try for yourself.\n",
        ":::\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {},
      "outputs": [
        {
          "ename": "ImportError",
          "evalue": "cannot import name 'SimpleDirectoryReader' from 'llama_index' (unknown location)",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)",
            "Cell \u001b[0;32mIn[1], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mllama_index\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m SimpleDirectoryReader, GPTVectorStoreIndex\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mollama\u001b[39;00m\n",
            "\u001b[0;31mImportError\u001b[0m: cannot import name 'SimpleDirectoryReader' from 'llama_index' (unknown location)"
          ]
        }
      ],
      "source": [
        "from llama_index.core import SimpleDirectoryReader, GPTVectorStoreIndex\n",
        "import ollama\n",
        " "
      ]
    },
    {
      "cell_type": "raw",
      "metadata": {},
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "base",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.11"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
