{
 "cells": [
  {
   "cell_type": "raw",
   "id": "d7c1ee20",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "---\n",
    "title: Command line tool to code review pull requests with AI running on your machine\n",
    "date: 2025-02-09\n",
    "draft: False\n",
    "publish: True\n",
    "callout-appearance: simple\n",
    "categories: [openai, ollama, command-line, click, github, llm, llm-code-review]\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "591f025b",
   "metadata": {},
   "source": [
    "Since I am too impatient to wait for access to [CoPilot powered code reviews](https://github.com/github-copilot/code-review-waitlist/), I  wrote a command-line tool that fetches pull requests from GitHub and code reviews them with AI. \n",
    "\n",
    "I'm also an enthusiast for running LLM models locally, so the tool allows users to select either models from OpenAI or local models installed with Ollama.\n",
    "\n",
    "Introducing: [llm-code-review](https://www.github.com/geirfreysson/llm-code-review)\n",
    "\n",
    "## Install\n",
    "```\n",
    "pip install llm-code-review\n",
    "```\n",
    "\n",
    "## How to use\n",
    "```\n",
    "llm-code-review [repo-owner]/[repo] [PR number]\n",
    "```\n",
    "For example, to get a code review from the first ever pull request to this repo, run:\n",
    "```\n",
    "llm-code-review geirfreysson/llm-code-review 1\n",
    "```\n",
    "\n",
    "## Run without installing with uv\n",
    "You can skip the `pip install` step if you have Astral's uv installed.\n",
    "```\n",
    "uvx llm-code-review geirfreysson/llm-code-review 1\n",
    "```\n",
    "\n",
    "## Run with local llm\n",
    "If you don't fancy sending your code to the cloud, you can use a local model, powered by Ollama.\n",
    "```\n",
    "llm-code-review octocat/Hello-World 42 --model ollama:deepseek-r1:8b\n",
    "```\n",
    "\n",
    "## Pipe your results into LLMs\n",
    "The AI produces a code review no matter how good or bad the code is, so sometimes the signal to noise can be a bit low. One way of dealing with that is to pipe the result of the code review into Simon Willison's LLM. Just call `llm` with the parameter `--system` along with your prompt.\n",
    "\n",
    "```\n",
    "$ uvx llm-code-review geirfreysson/llm-code-review 1 | llm --system \"Is this fix absolutely critical, i.e. will it break something, answer only with critical or not critical\"\n",
    "Not critical\n",
    "```\n",
    "\n",
    "No critical comments on the pull request. That's something.\n",
    "\n",
    "## Output from the code review\n",
    "\n",
    "The output is an looks something like this:\n",
    "\n",
    "![](code-review-screenshot.png)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
