{
 "cells": [
  {
   "cell_type": "raw",
   "id": "74f249ad",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "---\n",
    "title: Testing the biggest Deepseek model I can run on my Mac\n",
    "date: 2025-02-03\n",
    "draft: False\n",
    "publish: False\n",
    "image: deepseek-r1-screenshot.png\n",
    "callout-appearance: simple\n",
    "categories: [llm, webui, ollama, deepseek]\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d51fc7d0",
   "metadata": {},
   "source": [
    "I've been playing around with running Deepseek on my Mac, but until now I only opted for the 7b parameter model. When I came across this tweet from DHH I figured I should check what happens if I run a bigger model.\n",
    "\n",
    "{{< tweet dhh 1883960783983559044 >}}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fee839a5",
   "metadata": {},
   "source": [
    "```{.terminal}\n",
    "ollama pull deepseek-r1:32b\n",
    "```"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
