{
 "cells": [
  {
   "cell_type": "raw",
   "id": "d1691a25",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "---\n",
    "title: \"Command line tool to download code or functions to use as an AI prompt\"\n",
    "date: \"2025-02-22\"\n",
    "draft: false\n",
    "publish: true\n",
    "callout-appearance: simple\n",
    "categories: [command-line, click, github, llm-prompt]\n",
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c5e8e5c",
   "metadata": {},
   "source": [
    "Recently, I had an issue with two functions in my codebase that processed crosstabs and statistics to go along with them. One handled a parameter, `stats = ['mean']` correctly, while the other failed.\n",
    "\n",
    "I wanted to get the AI to compare these functions and suggest why one was working but the other one wasn't.\n",
    "\n",
    "Enter: [codegrab](https://github.com/geirfreysson/codegrab)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e87264c",
   "metadata": {},
   "source": [
    "## Codegrab\n",
    "With the help of ChatGPT I wrote a command line script where the user points to a github repository and the script pulls the python module, or function, from the repo.\n",
    "\n",
    "Codegrab also accepts data piped into it, in which case it adds the piped in code to the code being fetched. So, to compare the two functions and have an LLM assist me, I can write\n",
    "\n",
    "```{.terminal}\n",
    "codegrab module.location:function --repo [private repo] \\\n",
    " | codegrab other_module.location:function --repo [private repo] \\\n",
    " | llm \"The first function does not work, the second one does. The first one fails when I supply stats=['mean'] whereas the second one successfully includes stats. Can you see what the difference could be? Can you suggest changes to the first implementation so that it accepts stats in the same way the second one does?\"\n",
    "```\n",
    "\n",
    "It worked, the output pointed to why one function was working and the other wasn't.\n",
    "\n",
    "## Writing a test for codegrab using codegrab\n",
    "Here's another use case, grabbing the code for this new tool, codegrab itself, and having AI create tests for it.\n",
    "\n",
    "```{.terminal}\n",
    "codegrab codegrab --repo http://github.com/geirfreysson/codegrab \\\n",
    "| uvx llm \"Write tests for me for this module using pytest\" -m 4o\n",
    "```\n",
    "\n",
    "The tests the AI came up with mostly worked, but it took me 90 minutes or so to fix them so they all passed successfully. I've never used `mock` in pytest this way before to test remote requests, so it was a learning experience. Now, codegrab has [AI generated tests](https://github.com/geirfreysson/codegrab/blob/main/tests/test_codegrab.py), courtesy of codegrab.\n",
    "\n",
    "For installation and usage instructions, check out [the repo for codegrab](https://github.com/geirfreysson/codegrab).\n",
    "\n",
    ":::{.callout-note}\n",
    "You can also try [repomix](https://github.com/yamadashy/repomix) if you don't dislike the npm ecosystem as much as me, or [files-to-prompt](https://github.com/simonw/files-to-prompt) if you are happy to clone/download the repository before getting the code. \n",
    ":::"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d133efd",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
