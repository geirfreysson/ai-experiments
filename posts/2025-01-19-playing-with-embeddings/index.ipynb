{
 "cells": [
  {
   "cell_type": "raw",
   "id": "13e50ccf",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "---\n",
    "title: \"Understanding embeddings - how machines see language\"\n",
    "date: \"2025-01-19\"\n",
    "image: \"\" \n",
    "callout-appearance: simple\n",
    "categories: [embedding, langchain, ollama]\n",
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Embedding is the concept of representing words, concepts, and sentences so that computers can understand them. It's fundemental to LLMs and it also underpins things like semantic search, Netflix recomendations, and Google translate. I want to play around with embeddings a bit to see how they work and how they can be used. \n",
    "\n",
    "## Using local LLMs\n",
    "I'm more interested in using local LLM models than using ChatGPT or Claude, mostly because I work with client data that can't be sent out into the ether. I use [Ollama](https://ollama.com/), free software that let's you run large language models like Llama, Phi, Mistral, and Gemma on your local machine. \n",
    "\n",
    "After installing Ollama, open the terminal and type:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c5ece40",
   "metadata": {},
   "source": [
    "```` {.command-line .wrap}\n",
    "$ ollama pull llama3.1\n",
    "$ ollama run llama3.1\n",
    ">>> in one sentence, what is the meaning of life?\n",
    "The meaning of life is a subjective and often debated concept, but it can be distilled to finding purpose, happiness, and fulfillment through personal growth, relationships, and contributions that bring value to oneself and others.\n",
    "````"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ebca5dc",
   "metadata": {},
   "source": [
    "Seems about right, we now have an LLM running on our machine."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e58f989e",
   "metadata": {},
   "source": [
    "## Embedding\n",
    "When embedding language into a representation machines understand, we turn the words into vectors. \n",
    "\n",
    "For example, if we were using two dimensional vectors, we could visualise them in a 2D graph, shown below:\n",
    "\n",
    "![](vectors.svg){fig-align=\"center\"}\n",
    "\n",
    "Even though \"arrow\" and \"sparrow\" are spelled similarly and sound similar, their 2D vector representation is more different than the difference between sparrow and eagle. This difference is usually measured by calculating **cosine similarity**, essentially the angle from one vector to another.\n",
    "\n",
    "We are not going to embed them using 2 dimensions though, we are going to use 4,096. \n",
    "\n",
    "## Embedding with Ollama and LlangChain\n",
    "\n",
    "The easiest way I could find to play around with embeddings with ollama was [LangChain](https://www.langchain.com/), a toolkit meant to make application development with LLMs easier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "05414ded",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_ollama import OllamaEmbeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "2bd88148",
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddings = OllamaEmbeddings(\n",
    "    model=\"llama3.1\",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78056762",
   "metadata": {},
   "source": [
    "We are using Llama3.1 because it has a version with 8bn parameters that performs acceptably fast on my Mac.\n",
    "\n",
    "Now, let's embed some words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "c8f79d93",
   "metadata": {},
   "outputs": [],
   "source": [
    "sparrow_vector = embeddings.embed_query(\"sparrow\")\n",
    "arrow_vector = embeddings.embed_query(\"arrow\")\n",
    "eagle_vector = embeddings.embed_query(\"eagle\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93b8497a",
   "metadata": {},
   "source": [
    "Here is what the first 10 elements of the sparrow vector look like."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "f16e7b87",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[-0.0054991185,\n",
       " -0.026986344,\n",
       " 0.022912376,\n",
       " 0.014657578,\n",
       " 0.009402687,\n",
       " 0.0089849755,\n",
       " -0.016890066,\n",
       " 0.0144533245,\n",
       " 0.017101986,\n",
       " 0.0026423668]"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sparrow_vector[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb3ccfdd",
   "metadata": {},
   "source": [
    "Next, we use sklearn to calculate the cosine_similarity between the vector's we've just created."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "7f46226b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Similarity: \"sparrow\" and \"arrow\": 0.3617\n",
      "Similarity: \"sparrow\" and \"eagle\": 0.7927\n",
      "Similarity: \"arrow\" and \"eagle\": 0.3538\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "# Create a list of vectors\n",
    "the_words = [\"sparrow\", \"arrow\", \"eagle\"]\n",
    "# Calculate cosine similarity between each pair\n",
    "similarity_matrix = cosine_similarity([sparrow_vector, arrow_vector, eagle_vector])\n",
    "# Display the matrix\n",
    "\n",
    "def compare_words(words, matrix):\n",
    "    for i in range(len(words)):\n",
    "        for j in range(i + 1, len(words)):  # Start from i+1 to skip duplicate pairs\n",
    "            word1, word2 = words[i], words[j]\n",
    "            similarity_score = matrix[i, j]\n",
    "            print(f\"Similarity: \\\"{word1}\\\" and \\\"{word2}\\\": {similarity_score:.4f}\")\n",
    "compare_words(the_words, similarity_matrix)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad4f2b1e",
   "metadata": {},
   "source": [
    "As expected, the two birds are similar while the arrow is not. We can take this even further."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "6b88299d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Similarity: \"Man runs for office\" and \"Man runs for a bus\": 0.8682\n",
      "Similarity: \"Man runs for office\" and \"Woman becomes politician\": 0.9074\n",
      "Similarity: \"Man runs for a bus\" and \"Woman becomes politician\": 0.8527\n"
     ]
    }
   ],
   "source": [
    "sentences = [\n",
    "    \"The man with the tie ran for office.\",\n",
    "    \"The man with the tie ran for a bus.\",\n",
    "    \"The woman in the dress became a politician.\"\n",
    "]\n",
    "vectors = []\n",
    "for s in sentences:\n",
    "    vectors.append(embeddings.embed_query(s))\n",
    "similarity_matrix_v2 = cosine_similarity(vectors)\n",
    "compare_words([\"Man runs for office\", \"Man runs for a bus\", \"Woman becomes politician\"], similarity_matrix_v2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "370e4c9e",
   "metadata": {},
   "source": [
    "Again the man running for office is closer to the woman politician rather than the man running for the bus. Using our embeddings to find similar sentiments works!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd045867",
   "metadata": {},
   "source": [
    ":::{.callout-note}\n",
    "I usually give ChatGPT to code for me, but in the case of using Ollama to have local LLMs do embeddings, it was useless. I had to use Google (gasp) and find the LangChain documentation on [Ollama embeddings](https://python.langchain.com/docs/integrations/text_embedding/ollama/).\n",
    ":::"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
