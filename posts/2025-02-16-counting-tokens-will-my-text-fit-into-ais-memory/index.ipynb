{
 "cells": [
  {
   "cell_type": "raw",
   "id": "038806e3",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "---\n",
    "title: \"Counting Tokens: Will My Text Fit into AI's Memory?\"\n",
    "abstract: I count the tokens in a blog post, the complete works of Shakespeare, and the 500k+ emails from the Enron scandal and ask, is RAG dead?\n",
    "date: 2025-02-16\n",
    "draft: False\n",
    "publish: True\n",
    "callout-appearance: simple\n",
    "categories: [tokens, llm, enron]\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "acfb716f",
   "metadata": {},
   "source": [
    "## Tokens and context windows (or words and memory)\n",
    "In LLMs the equivalent of human words and memory are tokens and the context window. LLMs break words and sentences into tokens, and the size of the context window determines the maximum number of tokens an LLM can process at once.\n",
    "\n",
    "![](context-window.png){width=650  fig-align=center}\n",
    "\n",
    "The LLM will not remember any bit that is outside of the context window. If your conversation is too long, it will forget how it started."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88af21f6",
   "metadata": {},
   "source": [
    "To illustrate how tokens work, we use OpenAI's `tiktoken` library and break a sentence down into tokens."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "7476c0ab",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Super', 'cal', 'if', 'rag', 'il', 'istic', 'exp', 'ial', 'id', 'ocious', ' is', ' a', ' real', ' word', '.']\n"
     ]
    }
   ],
   "source": [
    "#| code-overflow: wrap\n",
    "import tiktoken\n",
    "# Choose the encoding based on the model\n",
    "encoding = tiktoken.encoding_for_model(\"gpt-4o\")\n",
    "text = \"Supercalifragilisticexpialidocious is a real word.\"\n",
    "\n",
    "# Tokenize\n",
    "tokens = encoding.encode(text)\n",
    "print([encoding.decode([t]) for t in tokens])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c11513fd",
   "metadata": {},
   "source": [
    "## Dealing with small context windows\n",
    "When ChatGPT launched in November 2022, the model it used had a context window of 2,048 tokens. For context, I  count the tokens in one of my [blog posts](/posts/2025-02-09-llm-app-to-search-through-pdf-documents/index.html) by piping the post into `strip-tags` and then into `ttok`, a command line wrapper for the token countign library I used above[^1]:\n",
    "```{.command-line}\n",
    "$ curl -s https://geirfreysson.com/posts/15-jan-2024-titanic/index.html \\\n",
    "   | uvx strip-tags .content \\\n",
    "   | uvx ttok\n",
    "\n",
    "2877\n",
    "```\n",
    "So, in the first version of ChatGPT, it would have forgotten about the beginnnig of my blog post as soon as it had ingested the whole post, leaving less than zero space for asking questions.\n",
    "\n",
    "[^1]: We use Simon Willison's `strip-tags` to strip HTML tags, `ttok` to count tokens, and `uvx` so we don't have to install the two packages. More on those two packages on [Simon's blog](https://simonwillison.net/2023/May/18/cli-tools-for-llms/).\n",
    "\n",
    "With a small context window, the only way to converse with your documents is to train the model on them or use RAG (Retrieval-Augmented Generation)[^2]. RAG splits documents into fragments, uses embeddings for semantic search[^3], and retrieves relevant parts before responding.\n",
    "\n",
    "[^3]: I explored embeddings in a [previous blog post](/posts/2025-01-19-playing-with-embeddings/index.html).\n",
    "\n",
    "Using RAG, your prompt would look like this:\n",
    "\n",
    "```\n",
    "[your search result fragment here] Please summarise this text for me.\n",
    "```\n",
    "\n",
    "[^2]: I built a basic RAG mechanism in a [previous post](/posts/2025-02-09-llm-app-to-search-through-pdf-documents/index.html).\n",
    "\n",
    "\n",
    "## Context windows have growm 1,000x\n",
    "LLMs are advancing, and the \"experimental\" version of Google's Gemini [reportedly](https://blog.google/technology/ai/google-gemini-next-generation-model-february-2024/) has a context window of 2 million tokens. If we now look at the complete works of Shakespeare (38 plays, 154 sonnets, two long narrative poems, and several other poems) and count its tokens\n",
    "\n",
    "```{.command-line}\n",
    "curl -s https://ocw.mit.edu/ans7870/6/6.006/s08/lecturenotes/files/t8.shakespeare.txt \\\n",
    ">   | uvx ttok\n",
    "1474814 \n",
    "```\n",
    "The Bard's complete works amount to 1.47M tokens, equivalent to 512 blog posts, and with a context window of 2M tokens, we still have plenty to spare if we stuff it into our prompt and start asking questions.\n",
    "\n",
    "Some have said this means RAG is dead, but for larger datasets there is still a place for it."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e46a85d9",
   "metadata": {},
   "source": [
    "## The Enron emails\n",
    "One dataset that fascinates me is the Enron email dataset, which contains over 500,000 emails from more than 150 employees. While I haven’t found a verified source, it’s been reported that someone used LLM techniques to uncover an email suggesting criminal activity - hidden within sarcasm. Because of the tone, a traditional keyword search wouldn’t have worked; only sentiment analysis could reveal it.\n",
    "\n",
    "Could we stuff the Enron emails into our Gemini prompt and start asking questions?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "461b92cb",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "f69ed733",
   "metadata": {},
   "source": [
    "### Download the Enron dataset\n",
    "Kaggle, the data science platform, has a lot of datasets available for download, so we use its library to fetch the Enron data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "bcad6092",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/Users/geirfreysson/.cache/kagglehub/datasets/wcukierski/enron-email-dataset/versions/2\n"
     ]
    }
   ],
   "source": [
    "import kagglehub\n",
    "\n",
    "# Download latest version\n",
    "path = kagglehub.dataset_download(\"wcukierski/enron-email-dataset\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e9499f2",
   "metadata": {},
   "source": [
    "I don't want to crash my meager MacBook, so I read and count the data in chunks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53ef7af6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total tokens in 'message': 425,303,390\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "def count_tokens(text, model=\"gpt-4o\"):\n",
    "    encoding = tiktoken.encoding_for_model(\"gpt-4o\")\n",
    "    return len(encoding.encode(text))\n",
    "\n",
    "# Define chunk size (e.g., 10,000 rows at a time)\n",
    "chunk_size = 10000\n",
    "total_tokens = 0\n",
    "\n",
    "# Read the CSV in chunks\n",
    "for chunk in pd.read_csv(path + \"/emails.csv\", usecols=[\"message\"], chunksize=chunk_size):\n",
    "    # Convert to string and count tokens in the batch\n",
    "    chunk[\"token_count\"] = chunk[\"message\"].astype(str).map(count_tokens)\n",
    "    \n",
    "    # Sum up tokens in the chunk\n",
    "    total_tokens += chunk[\"token_count\"].sum()\n",
    "\n",
    "print(f\"Total tokens in '{column_name}': {total_tokens:,}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "803f0f1b",
   "metadata": {},
   "source": [
    ":::{.callout-warning}\n",
    "Warning: This number seems high. If the average email had 100 words, I’d expect around 50M tokens, not 425M. There might be a bug in the code, or the high count could be because the metadata is included in the count along the actual email content.\n",
    ":::"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca954eeb",
   "metadata": {},
   "source": [
    "## To RAG or not to RAG\n",
    "The Enron data is too big to stuff all of it into a prompt and start typing, even for Google's latest 2M token context window. Does that mean RAG is the solution? Maybe. Or should we train a model using the Enron data and see if it can find the incriminating evidence that way? Possibly. That's for a future post.\n",
    "\n",
    ":::{.callout-note}\n",
    "Astral's uv turns one year old today. When I counted the tokens using the command line, I used strip-tags and ttok, but thanks to uv I didn't need to install them, I just used uvx. Check out uv now if you haven't already.\n",
    ":::"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
