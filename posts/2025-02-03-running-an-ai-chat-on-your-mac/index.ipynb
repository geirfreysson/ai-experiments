{
 "cells": [
  {
   "cell_type": "raw",
   "id": "74f249ad",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "---\n",
    "title: \"Chat with the hottest LLM model on your Mac via the browser\"\n",
    "date: \"2025-02-03\"\n",
    "draft: false\n",
    "publish: false\n",
    "image: \"deepseek-r1-screenshot.png\" \n",
    "callout-appearance: simple\n",
    "categories: [llm, webui, ollama, deepseek]\n",
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91f86905",
   "metadata": {},
   "source": [
    "LLM models that run on your local machine are becoming better and better. Indeed, thanks to the AI arms race, these models will soon be good enough for a lot of the tasks required by individuals and companies that want to leverage language models. I've [already covered](/posts/2025-01-19-playing-with-embeddings/index.html#using-local-llms) how to install a LLM locally using Ollama, but how do you chat with the model in a browser? Enter WebUI.\n",
    "\n",
    "## Installing WebUI\n",
    "[WebUI](https://openwebui.com/) is an open source project that describes itself as\n",
    "\n",
    "> an extensible, self-hosted AI interface that adapts to your workflow, all while operating entirely offline\n",
    "\n",
    "Their first recommended installation option, using Docker, didn't work for me, but luckily they provide a guide on how to install it via the new python package manager uv.\n",
    "\n",
    "Getting started is as simple as opening your terminal and pasting this in:\n",
    "\n",
    "```{.terminal}\n",
    "DATA_DIR=~/.open-webui uvx --python 3.11 open-webui@latest serve\n",
    "```\n",
    "\n",
    ":::{.callout-note}\n",
    "The easiest way to chat with a local model is probably [LM Studio](https://lmstudio.ai/). It integrates a GUI with the ability to download models via a point-and-click interface. I'm more interested in an option that can become part of a product and is more customisable.\n",
    ":::\n",
    "\n",
    "### It's alive\n",
    "After waiting for everything to download, the server is running\n",
    "\n",
    "![](webui-screenshot.png)\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
