{
 "cells": [
  {
   "cell_type": "raw",
   "id": "e441626a",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "---\n",
    "title: \"Searching PDF documents using LLMs with RAG\"\n",
    "abstract: \"Embeddings can be used to search documents with results that reflect the meaning of your search rather than the actual search words and terms. In this post, I implement a RAG search with the help of LangChain and embeddings.\"\n",
    "date: \"2025-02-08\"\n",
    "draft: false\n",
    "publish: true\n",
    "callout-appearance: simple\n",
    "categories: [langchain, RAG, embeddings, openai, faiss]\n",
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "819c6580",
   "metadata": {},
   "source": [
    "One of the interesting things about embeddings[^1] is how they can be used to search documents with results that reflect the meaning of your search rather than the actual search words and terms.\n",
    "\n",
    "[^1]: AWS explains embeddings [here](https://aws.amazon.com/what-is/embeddings-in-machine-learning) and I play around with them [here](../2025-01-19-playing-with-embeddings/index.html).\n",
    "\n",
    "I am going to use Latent Space’s [The 2025 AI Engineer Reading List](https://www.latent.space/p/2025-papers) and search through the PDFs in their sections about RAG and Agents to see what happens when I search for RAG related terms.\n",
    "\n",
    ":::{.callout-note}\n",
    "The technique explored below may have become redundant for smaller collections of files like the ones used below, thanks to models like [Gemini 2.o Pro Experimental](https://blog.google/technology/google-deepmind/gemini-model-updates-february-2025/). It has a context window of 2 million tokens, which allows you to load much more data into the prompt before engaging with the model.\n",
    ":::"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "8d0d95c8",
   "metadata": {
    "tags": [
     "hide-cell"
    ]
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/th/3vhglq3x3sg02czn_l49xpvc0000gn/T/ipykernel_3072/2849008883.py:7: DeprecationWarning: Importing display from IPython.core.display is deprecated since IPython 7.14, please import from IPython display\n",
      "  from IPython.core.display import display, HTML\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain.document_loaders import PyPDFLoader\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain_openai import OpenAIEmbeddings\n",
    "from langchain.vectorstores import FAISS\n",
    "from langchain_openai import OpenAI\n",
    "from langchain.chains.summarize import load_summarize_chain\n",
    "from IPython.core.display import display, HTML\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()  # take environment variables from .env."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f498295",
   "metadata": {},
   "source": [
    "## Using LangChain to Connect LLMs with Your Data\n",
    "I'm using LangChain, an open source framework that makes life easier for people writing LLM applications. We start by looping through the documents and use `PyPDFLoader` and `RecursiveCharacterTextSplitter`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "06609524",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| source-line-numbers: \"7,11,15\"\n",
    "pdf_directory = \"../../sample_data/The 2025 AI Engineer Reading List/\"\n",
    "pdf_files = [f for f in os.listdir(pdf_directory) if f.endswith('.pdf')]\n",
    "\n",
    "documents = []\n",
    "for pdf_file in pdf_files:\n",
    "    file_path = os.path.join(pdf_directory, pdf_file)\n",
    "    loader = PyPDFLoader(file_path)\n",
    "    documents.extend(loader.load())\n",
    "\n",
    "# Optionally split the documents into manageable chunks.\n",
    "text_splitter = RecursiveCharacterTextSplitter(\n",
    "    chunk_size=1000, \n",
    "    chunk_overlap=200\n",
    ")\n",
    "docs = text_splitter.split_documents(documents)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "485d0442",
   "metadata": {},
   "source": [
    "The `RecursiveCharacterTextSplitter` creates a list of document objects, storing their meta-data (document name, content page number, etc.) along with the actual content. Note that this is still stored as normal text at this stage.\n",
    "\n",
    "The `chunk size` parameter means how thin the slices are in our cake (and how many slices we'll end up with). Overlap controls how much content from one slice is carried into the next, ensuring continuity in meaning. These need to be tweaked according to what kind of search we are implementing. Frankly, I’m clueless at this stage what is the best setting for my PDF search."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a89486f4",
   "metadata": {},
   "source": [
    "## Create embeddings for the documents\n",
    "We need our AI models to be able to work with the documents, so we create embeddings. This is where we start hitting the ChatGPT API. I would prefer to use a local LLM, but for now we need the best results possible to know that if there are problems, they're my fault and not the model's.\n",
    "\n",
    "We use embeddings from OpenAI and store them in FAISS, a library developed by Meta for efficient similarity search. The score ranges from 0 to X, where lower scores signify a closer match in Euclidean distance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "965589f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddings = OpenAIEmbeddings()  # needs the OPENAI_API_KEY environment variable\n",
    "vectorstore = FAISS.from_documents(docs, embeddings)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d440802a",
   "metadata": {},
   "source": [
    "And if we peek into the vectorstore object and look at our first embedding, we see how the text is represented as a vector/array."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "95688e75",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-0.01694826, -0.00412454, -0.00136518, ..., -0.00767243,\n",
       "        0.00234885, -0.04821463], dtype=float32)"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vectorstore.index.reconstruct(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e41618a4",
   "metadata": {},
   "source": [
    "## Do the actual search"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d15a814",
   "metadata": {},
   "source": [
    "We use FAISS's `similarity_search_with_score` which not only calculates similarity but also returns a similarity score."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "bfb8ed6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"using llms to search documents\"\n",
    "retrieved_docs = vectorstore.similarity_search_with_score(\n",
    "    query= query, \n",
    "    k=10   # how many docs to return\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20d77d7a",
   "metadata": {},
   "source": [
    "If we look at the filenames returned, we see that the same file had high scoring sections/pages and is in places 2-4."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "1515d883",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['From Local to Global - A Graph RAG Approach to Query-Focused Summarization.pdf',\n",
       " 'MemGPT - Towards LLMs as Operating Systems.pdf',\n",
       " 'MemGPT - Towards LLMs as Operating Systems.pdf',\n",
       " 'MemGPT - Towards LLMs as Operating Systems.pdf',\n",
       " 'Building effective agents _ Anthropic.pdf',\n",
       " 'REACT - SYNERGIZING REASONING AND ACTING IN LANGUAGE MODELS.pdf',\n",
       " 'RAGAS - Automated Evaluation of Retrieval Augmented Generation.pdf',\n",
       " 'MemGPT - Towards LLMs as Operating Systems.pdf',\n",
       " 'From Local to Global - A Graph RAG Approach to Query-Focused Summarization.pdf',\n",
       " 'MemGPT - Towards LLMs as Operating Systems.pdf']"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[i[0].metadata['source'].split(\"/\")[-1] for i in retrieved_docs]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6fffb721",
   "metadata": {},
   "source": [
    ":::{.callout-note}\n",
    "Note that we are using FAISS, which defaults to Euclidean distance unless configured to use cosine similarity, which we used in [previous](posts/2025-01-19-playing-with-embeddings/index.html) [experiments](/posts/2025-01-25-comparing-embedding-models/index.html).\n",
    ":::"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "e1a470c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4. For each retrieved document, use an LLM to summarize why it might be relevant.\n",
    "llm = OpenAI(temperature=0)  # low temperature for deterministic output\n",
    "\n",
    "def summarize_relevance(doc, query):\n",
    "    # Here we build a simple prompt to ask why the document is relevant to the query.\n",
    "    prompt = (\n",
    "        f\"Given the following text from a document:\\n\\n\"\n",
    "        f\"{doc.page_content}\\n\\n\"\n",
    "        f\"And the search query:\\n\\n\"\n",
    "        f\"{query}\\n\\n\"\n",
    "        f\"Summarize why this text is relevant to the query.\"\n",
    "    )\n",
    "    summary = llm.invoke(prompt)\n",
    "    return summary.strip()\n",
    "\n",
    "result = []\n",
    "for i, doc in enumerate(retrieved_docs[:4]):\n",
    "    relevance_summary = summarize_relevance(doc[0], query)\n",
    "    source_filename = doc[0].metadata['source'].split(\"/\")[-1]\n",
    "    excerpt = doc[0].page_content[:300] + \"...\"  # First 300 characters as excerpt\n",
    "    result.append({\n",
    "        \"relevance_summary\": relevance_summary,\n",
    "        \"source_filename\":source_filename,\n",
    "        \"excerpt\":excerpt,\n",
    "        \"score\":round(float(doc[1]),2)\n",
    "    })\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "808afd5f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "<table class=\"table plain\" border=\"1\" style=\"text-align:left; border-collapse: collapse; width: 100%; text-align: left;\">\n",
       "    <tr style=\"background-color: #f2f2f2;\">\n",
       "        <th>Relevance Summary</th>\n",
       "        <th>Excerpt</th>\n",
       "    </tr>\n",
       "\n",
       "    <tr style=\"background:white;\">\n",
       "    <td style=\"text-align:left;--bs-table-bg-type:none; font-weight:bold;\" colspan=\"2\">From Local to Global - A Graph RAG Approach to Query-Focused Summarization.pdf (score 0.37)</td>\n",
       "    </tr>\n",
       "    <tr style=\"background:white;\">\n",
       "        <td style=\"text-align:left; background-color:white;\">The text discusses the use of LLMs (Language Model Metrics) for evaluating natural language generation and conventional RAG (Retrieval-Augmented Generation) systems. It mentions that LLMs have been shown to be effective in measuring the qualities of generated texts and evaluating the performance of RAG systems. This is relevant to the query as it discusses the use of LLMs in searching and evaluating documents.</td>\n",
       "        <td style=\"text-align:left; font-style:italic\">\"relationship extraction only, with entity types and few-shot examples tailored to the domain of the\n",
       "data. The graph indexing process used a context window size of 600 tokens with 1 gleaning for the\n",
       "Podcast dataset and 0 gleanings for the News dataset.\n",
       "3.4 Metrics\n",
       "LLMs have been shown to be good eval...\"</td>\n",
       "    </tr>\n",
       "    \n",
       "    <tr style=\"background:white;\">\n",
       "    <td style=\"text-align:left;--bs-table-bg-type:none; font-weight:bold;\" colspan=\"2\">MemGPT - Towards LLMs as Operating Systems.pdf (score 0.39)</td>\n",
       "    </tr>\n",
       "    <tr style=\"background:white;\">\n",
       "        <td style=\"text-align:left; background-color:white;\">The text mentions using an LLM judge to evaluate whether an LLM correctly answered a question. This is relevant to the search query as it involves using LLMs to search documents. Additionally, the text discusses the format of the LLM response and the true answer, which could provide insight into how LLMs are used for document search.</td>\n",
       "        <td style=\"text-align:left; font-style:italic\">\"than from the model weights), we used an LLM judge. The\n",
       "LLM judge was provided the answers generated by both\n",
       "baseline approaches and MemGPT, and asked to make a\n",
       "judgement with the following prompt:\n",
       "Your task is to evaluate whether an LLM\n",
       "correct answered a question. The LLM\n",
       "response should be the fo...\"</td>\n",
       "    </tr>\n",
       "    \n",
       "    <tr style=\"background:white;\">\n",
       "    <td style=\"text-align:left;--bs-table-bg-type:none; font-weight:bold;\" colspan=\"2\">MemGPT - Towards LLMs as Operating Systems.pdf (score 0.4)</td>\n",
       "    </tr>\n",
       "    <tr style=\"background:white;\">\n",
       "        <td style=\"text-align:left; background-color:white;\">The text discusses the limitations of using LLMs (language model models) for conversational AI and their inability to handle long conversations or documents. It also mentions the challenges of extending the context length of transformers, which are used in LLMs, and the need for new long-context architectures. This is relevant to the search query as it highlights the use of LLMs in searching documents and the challenges associated with it.</td>\n",
       "        <td style=\"text-align:left; font-style:italic\">\"have become the cornerstone of conversational AI and have\n",
       "led to a wide array of consumer and enterprise applications.\n",
       "Despite these advances, the limited fixed-length context\n",
       "windows used by LLMs significantly hinders their appli-\n",
       "cability to long conversations or reasoning about long doc-\n",
       "uments. ...\"</td>\n",
       "    </tr>\n",
       "    \n",
       "    <tr style=\"background:white;\">\n",
       "    <td style=\"text-align:left;--bs-table-bg-type:none; font-weight:bold;\" colspan=\"2\">MemGPT - Towards LLMs as Operating Systems.pdf (score 0.4)</td>\n",
       "    </tr>\n",
       "    <tr style=\"background:white;\">\n",
       "        <td style=\"text-align:left; background-color:white;\">ANSWER: The text discusses the use of an LLM judge to check the correctness of answers generated by baseline approaches and MemGPT for document analysis tasks. DOCUMENT: The text is relevant to the query because it specifically mentions the use of LLMs in searching documents, which is the main focus of the query. It also provides information on how the LLM judge was used to ensure that the answer was derived from the provided text rather than from model weights.</td>\n",
       "        <td style=\"text-align:left; font-style:italic\">\"response, provide both the answer\n",
       "and the document text from which you\n",
       "determined the answer. Format your\n",
       "response with the format ’ANSWER: <YOUR\n",
       "ANSWER>, DOCUMENT: [DOCUMENT TEXT]’. If\n",
       "none of the documents provided have\n",
       "the answer to the question, reply\n",
       "with ’INSUFFICIENT INFORMATION’. Do\n",
       "NOT prov...\"</td>\n",
       "    </tr>\n",
       "    </table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#| code-fold: true\n",
    "#| code-summary: \"Show the code\"\n",
    "\n",
    "html_output = \"\"\"\n",
    "<table class=\"table plain\" border=\"1\" style=\"text-align:left; border-collapse: collapse; width: 100%; text-align: left;\">\n",
    "    <tr style=\"background-color: #f2f2f2;\">\n",
    "        <th>Relevance Summary</th>\n",
    "        <th>Excerpt</th>\n",
    "    </tr>\n",
    "\"\"\"\n",
    "\n",
    "for i in result:\n",
    "    html_output += f\"\"\"\n",
    "    <tr style=\"background:white;\">\n",
    "    <td style=\"text-align:left;--bs-table-bg-type:none; font-weight:bold;\" colspan=\"2\">{i[\"source_filename\"]} (score {round(float(i[\"score\"]),2)})</td>\n",
    "    </tr>\n",
    "    <tr style=\"background:white;\">\n",
    "        <td style=\"text-align:left; background-color:white;\">{i[\"relevance_summary\"]}</td>\n",
    "        <td style=\"text-align:left; font-style:italic\">\"{i[\"excerpt\"]}\"</td>\n",
    "    </tr>\n",
    "    \"\"\"\n",
    "\n",
    "html_output += \"</table>\"\n",
    "display(HTML(html_output))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c72c55aa",
   "metadata": {},
   "source": [
    "## Final thoughts\n",
    "At this point, I’m not entirely sure how good the search actually is. Some results seem relevant, but I don’t know how much of that is due to the tech itself versus my choice of parameters. The documents all cover a very similar topic, which makes it even harder to evaluate the quality of the results.\n",
    "\n",
    "The next step would be to try out a larger set of documents, tweaking parameters like chunk size and overlap and trying cosine similarity rather than Euclidean distance.\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
