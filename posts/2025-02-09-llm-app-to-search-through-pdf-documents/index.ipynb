{
 "cells": [
  {
   "cell_type": "raw",
   "id": "e441626a",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "---\n",
    "title: \"Searching PDF documents using LLMs\"\n",
    "date: \"2025-02-09\"\n",
    "draft: \"false\"\n",
    "publish: \"false\"\n",
    "image: \"\" \n",
    "callout-appearance: simple\n",
    "categories: []\n",
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "819c6580",
   "metadata": {},
   "source": [
    "One of the interesting things about embeddings[^1] is how they can be used to search documents with results that reflect the meaning of your search rather than the actual search words and terms.\n",
    "\n",
    "[^1]: AWS explains embeddings [here](https://aws.amazon.com/what-is/embeddings-in-machine-learning) and I play around with them [here](../2025-01-19-playing-with-embeddings/index.html).\n",
    "\n",
    "I am going to use Latent Space’s [The 2025 AI Engineer Reading List](https://www.latent.space/p/2025-papers) and search through the PDFs in their sections about RAG and Agents to see what happens when I search for RAG related terms."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8d0d95c8",
   "metadata": {
    "tags": [
     "hide-cell"
    ]
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain.document_loaders import PyPDFLoader\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain_openai import OpenAIEmbeddings\n",
    "from langchain.vectorstores import FAISS\n",
    "from langchain_openai import OpenAI\n",
    "from langchain.chains.summarize import load_summarize_chain\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()  # take environment variables from .env."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f498295",
   "metadata": {},
   "source": [
    "## LLM application development made easier with LangChain\n",
    "I'm using LangChain, an open source framework that makes life easier for people writing LLM applications."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "06609524",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| source-line-numbers: \"7,8,11,12\"\n",
    "pdf_directory = \"../../sample_data/The 2025 AI Engineer Reading List/RAG/\"\n",
    "pdf_files = [f for f in os.listdir(pdf_directory) if f.endswith('.pdf')]\n",
    "\n",
    "documents = []\n",
    "for pdf_file in pdf_files:\n",
    "    file_path = os.path.join(pdf_directory, pdf_file)\n",
    "    loader = PyPDFLoader(file_path)\n",
    "    documents.extend(loader.load())\n",
    "\n",
    "# Optionally split the documents into manageable chunks.\n",
    "text_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=200)\n",
    "docs = text_splitter.split_documents(documents)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "485d0442",
   "metadata": {},
   "source": [
    "The `RecursiveCharacterTextSplitter` creates a list of document objects, storing their meta-data (document name, page number of content etc) along with the actual content. Note that this is still stored as normal text at this tage."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a89486f4",
   "metadata": {},
   "source": [
    "## Create embeddings for the documents\n",
    "We need our AI models to be able to work with the documents, so we create embeddings. This is where we start hitting the ChatGPT API. I would prefer to use a local LLM, but for now we need the best results possible to know where our code is doing a good job and where it isn't. \n",
    "\n",
    "We use embeddings from OpenAI and Meta's FAISS library for searching for similar phrases."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "965589f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddings = OpenAIEmbeddings()  # needs the OPENAI_API_KEY environment variable\n",
    "vectorstore = FAISS.from_documents(docs, embeddings)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d776b466",
   "metadata": {},
   "source": [
    "And if we explore the embeddings, there are a total 362 embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "aed8f659",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "362"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vectorstore.index.ntotal"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f5f1bdb",
   "metadata": {},
   "source": [
    "where each embedding has 1536 dimensions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "95688e75",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-0.02747275, -0.01618657, -0.0022142 , ..., -0.02544596,\n",
       "       -0.00838481, -0.03731519], shape=(1536,), dtype=float32)"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# look at the first embeeding\n",
    "vectorstore.index.reconstruct(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e41618a4",
   "metadata": {},
   "source": [
    "## Do the actual search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "bfb8ed6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"retreived augmented generation in large lanugage models, RAG\"\n",
    "retrieved_docs = vectorstore.similarity_search(query, k=5)  # adjust k as needed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e1a470c8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Document 1:\n",
      "--- Relevance Summary ---\n",
      "The text discusses the use of RAG models, which utilize input sequences to retrieve text documents and generate responses. These models have achieved state-of-the-art results on various tasks, including fact verification and question generation. The text also mentions the use of non-parametric memory to update the models' knowledge as the world changes. This is relevant to the query as it discusses the use of \"retreived augmented generation\" in large language models, which is a key aspect of RAG models.\n",
      "\n",
      "Document Excerpt:\n",
      "without access to an external knowledge source. Our RAG models achieve state-of-the-art results\n",
      "on open Natural Questions [29], WebQuestions [3] and CuratedTrec [2] and strongly outperform\n",
      "recent approaches that use specialised pre-training objectives on TriviaQA [24]. Despite these being\n",
      "extractive tasks, we ﬁnd that unconstrained generation outperforms previous extractive approaches.\n",
      "For knowledge-intensive generation, we experiment with MS-MARCO [1] and Jeopardy question\n",
      "generation, and we ﬁn ...\n",
      "\n",
      "================================================================================\n",
      "Document 2:\n",
      "--- Relevance Summary ---\n",
      "The text discusses the use of retrieval-augmented generation (RAG) in large language models, which is directly related to the search query. It explains that RAG involves retrieving relevant passages from a corpus and feeding them to a language model, and that recent work has shown that this approach can be successful. However, the implementation of RAG requires careful tuning and consideration of various factors, such as the retrieval model, corpus, and prompt formulation.\n",
      "\n",
      "Document Excerpt:\n",
      "Generation (RAG) (Lee et al., 2019; Lewis et al.,\n",
      "2020; Guu et al., 2020). Answering a question\n",
      "then essentially involves retrieving relevant pas-\n",
      "sages from a corpus and feeding these passages,\n",
      "along with the original question, to the LM. While\n",
      "initial approaches relied on specialised LMs for\n",
      "retrieval-augmented language modelling (Khandel-\n",
      "wal et al., 2020; Borgeaud et al., 2022), recent work\n",
      "has suggested that simply adding retrieved docu-\n",
      "ments to the input of a standard LM can also work\n",
      "wel ...\n",
      "\n",
      "================================================================================\n",
      "Document 3:\n",
      "--- Relevance Summary ---\n",
      "The text discusses a framework called RAGAS (Retrieval Augmented Generation Assessment) which is used for evaluating Retrieval Augmented Generation (RAG) pipelines. These pipelines consist of a retrieval system and a large language model (LLM) based generation module. The framework is designed to assess the ability of the retrieval system to identify relevant and focused context passages, as well as the ability of the LLM to utilize this information. This is directly relevant to the search query as it mentions \"retreived augmented generation in large language models\" and \"RAG\", which are both key components of the RAGAS framework.\n",
      "\n",
      "Document Excerpt:\n",
      "RAGAS: Automated Evaluation of Retrieval Augmented Generation\n",
      "Shahul Es†, Jithin James†, Luis Espinosa-Anke∗♢, Steven Schockaert∗\n",
      "†Exploding Gradients\n",
      "∗CardiffNLP, Cardiff University, United Kingdom\n",
      "♢AMPLYFI, United Kingdom\n",
      "shahules786@gmail.com,jamesjithin97@gmail.com\n",
      "{espinosa-ankel,schockaerts1}@cardiff.ac.uk\n",
      "Abstract\n",
      "We introduce RAGA S (Retrieval Augmented\n",
      "Generation Assessment), a framework for\n",
      "reference-free evaluation of Retrieval Aug-\n",
      "mented Generation (RAG) pipelines. RAG\n",
      "systems are c ...\n",
      "\n",
      "================================================================================\n",
      "Document 4:\n",
      "--- Relevance Summary ---\n",
      "The text discusses the importance of evaluating retrieval-augmented systems, specifically in terms of language modeling and question answering tasks. It also mentions the limitations of current evaluation strategies and introduces a new framework, RAGA S1, for automated assessment. This is relevant to the query as it pertains to the use of retrieval-augmented generation in large language models, specifically RAG systems.\n",
      "\n",
      "Document Excerpt:\n",
      "a significant amount of tuning, as the overall per-\n",
      "formance will be affected by the retrieval model,\n",
      "the considered corpus, the LM, or the prompt for-\n",
      "mulation, among others. Automated evaluation of\n",
      "retrieval-augmented systems is thus paramount. In\n",
      "practice, RAG systems are often evaluated in terms\n",
      "of the language modelling task itself, i.e. by mea-\n",
      "suring perplexity on some reference corpus. How-\n",
      "ever, such evaluations are not always predictive\n",
      "of downstream performance (Wang et al., 2023c).\n",
      "M ...\n",
      "\n",
      "================================================================================\n",
      "Document 5:\n",
      "--- Relevance Summary ---\n",
      "The text discusses the effectiveness of RAG models in generating diverse and factual results compared to BART models. It also mentions the use of retrieval mechanisms in RAG models and how it improves results for various tasks. This is relevant to the query as it mentions the use of RAG models in generating augmented results in large language models, which is the focus of the search query.\n",
      "\n",
      "Document Excerpt:\n",
      "in 71% of cases, and a gold article is present in the top 10 retrieved articles in 90% of cases.\n",
      "4.5 Additional Results\n",
      "Generation Diversity Section 4.3 shows that RAG models are more factual and speciﬁc than\n",
      "BART for Jeopardy question generation. Following recent work on diversity-promoting decoding\n",
      "[33, 59, 39], we also investigate generation diversity by calculating the ratio of distinct ngrams to\n",
      "total ngrams generated by different models. Table 5 shows that RAG-Sequence’s generations are\n",
      "mo ...\n",
      "\n",
      "================================================================================\n"
     ]
    }
   ],
   "source": [
    "# 4. For each retrieved document, use an LLM to summarize why it might be relevant.\n",
    "llm = OpenAI(temperature=0)  # low temperature for deterministic output\n",
    "\n",
    "def summarize_relevance(doc, query):\n",
    "    # Here we build a simple prompt to ask why the document is relevant to the query.\n",
    "    prompt = (\n",
    "        f\"Given the following text from a document:\\n\\n\"\n",
    "        f\"{doc.page_content}\\n\\n\"\n",
    "        f\"And the search query:\\n\\n\"\n",
    "        f\"{query}\\n\\n\"\n",
    "        f\"Summarize why this text is relevant to the query.\"\n",
    "    )\n",
    "    summary = llm.invoke(prompt)\n",
    "    return summary.strip()\n",
    "\n",
    "# Process and print the results\n",
    "for i, doc in enumerate(retrieved_docs):\n",
    "    relevance_summary = summarize_relevance(doc, query)\n",
    "    print(f\"Document {i+1}:\")\n",
    "    print(f\"--- Relevance Summary ---\\n{relevance_summary}\\n\")\n",
    "    print(\"Document Excerpt:\")\n",
    "    print(doc.page_content[:500], \"...\\n\")  # print first 500 characters as an excerpt\n",
    "    print(\"=\" * 80)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
