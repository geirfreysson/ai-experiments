{
 "cells": [
  {
   "cell_type": "raw",
   "id": "e441626a",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "---\n",
    "title: \"Searching PDF documents using LLMs\"\n",
    "date: \"2025-02-09\"\n",
    "draft: \"true\"\n",
    "publish: \"false\"\n",
    "callout-appearance: simple\n",
    "categories: []\n",
    "format:\n",
    "  html:\n",
    "    html-table-processing: none\n",
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "819c6580",
   "metadata": {},
   "source": [
    "One of the interesting things about embeddings[^1] is how they can be used to search documents with results that reflect the meaning of your search rather than the actual search words and terms.\n",
    "\n",
    "[^1]: AWS explains embeddings [here](https://aws.amazon.com/what-is/embeddings-in-machine-learning) and I play around with them [here](../2025-01-19-playing-with-embeddings/index.html).\n",
    "\n",
    "I am going to use Latent Space‚Äôs [The 2025 AI Engineer Reading List](https://www.latent.space/p/2025-papers) and search through the PDFs in their sections about RAG and Agents to see what happens when I search for RAG related terms."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "8d0d95c8",
   "metadata": {
    "tags": [
     "hide-cell"
    ]
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/th/3vhglq3x3sg02czn_l49xpvc0000gn/T/ipykernel_3072/2849008883.py:7: DeprecationWarning: Importing display from IPython.core.display is deprecated since IPython 7.14, please import from IPython display\n",
      "  from IPython.core.display import display, HTML\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain.document_loaders import PyPDFLoader\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain_openai import OpenAIEmbeddings\n",
    "from langchain.vectorstores import FAISS\n",
    "from langchain_openai import OpenAI\n",
    "from langchain.chains.summarize import load_summarize_chain\n",
    "from IPython.core.display import display, HTML\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()  # take environment variables from .env."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f498295",
   "metadata": {},
   "source": [
    "## LangChain: Connect LLMs with your data\n",
    "I'm using LangChain, an open source framework that makes life easier for people writing LLM applications. We start by looping through the documents and use `PyPDFLoader` and `RecursiveCharacterTextSplitter`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "06609524",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| source-line-numbers: \"7,11,15\"\n",
    "pdf_directory = \"../../sample_data/The 2025 AI Engineer Reading List/\"\n",
    "pdf_files = [f for f in os.listdir(pdf_directory) if f.endswith('.pdf')]\n",
    "\n",
    "documents = []\n",
    "for pdf_file in pdf_files:\n",
    "    file_path = os.path.join(pdf_directory, pdf_file)\n",
    "    loader = PyPDFLoader(file_path)\n",
    "    documents.extend(loader.load())\n",
    "\n",
    "# Optionally split the documents into manageable chunks.\n",
    "text_splitter = RecursiveCharacterTextSplitter(\n",
    "    chunk_size=1000, \n",
    "    chunk_overlap=200\n",
    ")\n",
    "docs = text_splitter.split_documents(documents)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "485d0442",
   "metadata": {},
   "source": [
    "The `RecursiveCharacterTextSplitter` creates a list of document objects, storing their meta-data (document name, content page number, etc.) along with the actual content. Note that this is still stored as normal text at this tage.\n",
    "\n",
    "The `chunk size` parameter means how thin the slices are in our cake (and how many slices we'll end up with) and the `overlap` is how much of one document bleeds into the next. These need to be tweaked according to what kind of search we are implementing. Frankly, I'm clueless at this tage what is the best setting for my PDF search."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a89486f4",
   "metadata": {},
   "source": [
    "## Create embeddings for the documents\n",
    "We need our AI models to be able to work with the documents, so we create embeddings. This is where we start hitting the ChatGPT API. I would prefer to use a local LLM, but for now we need the best results possible to know that if there are problems, they're my fault and not the model's.\n",
    "\n",
    "We use embeddings from OpenAI, and Meta's FAISS library to store and compare the vectors generated by the embedding. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "965589f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddings = OpenAIEmbeddings()  # needs the OPENAI_API_KEY environment variable\n",
    "vectorstore = FAISS.from_documents(docs, embeddings)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d440802a",
   "metadata": {},
   "source": [
    "And if we peek into the vectorstore object and look at our first embedding, we see how the text is represented as a vector/array."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "95688e75",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-0.01694826, -0.00412454, -0.00136518, ..., -0.00767243,\n",
       "        0.00234885, -0.04821463], dtype=float32)"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vectorstore.index.reconstruct(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e41618a4",
   "metadata": {},
   "source": [
    "## Do the actual search"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d15a814",
   "metadata": {},
   "source": [
    "We use FAISS's `similarity_search_with_score` which not only calculates similarity but also returns a similarity score."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "bfb8ed6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"using llms to search documents\"\n",
    "retrieved_docs = vectorstore.similarity_search_with_score(\n",
    "    query= query, \n",
    "    k=10   # how many docs to return\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20d77d7a",
   "metadata": {},
   "source": [
    "If we look at the filenames returned, we see that the same file had high scoring sections/pages and is in places 2-4."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "1515d883",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['From Local to Global - A Graph RAG Approach to Query-Focused Summarization.pdf',\n",
       " 'MemGPT - Towards LLMs as Operating Systems.pdf',\n",
       " 'MemGPT - Towards LLMs as Operating Systems.pdf',\n",
       " 'MemGPT - Towards LLMs as Operating Systems.pdf',\n",
       " 'Building effective agents _ Anthropic.pdf',\n",
       " 'REACT - SYNERGIZING REASONING AND ACTING IN LANGUAGE MODELS.pdf',\n",
       " 'RAGAS - Automated Evaluation of Retrieval Augmented Generation.pdf',\n",
       " 'MemGPT - Towards LLMs as Operating Systems.pdf',\n",
       " 'From Local to Global - A Graph RAG Approach to Query-Focused Summarization.pdf',\n",
       " 'MemGPT - Towards LLMs as Operating Systems.pdf']"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[i[0].metadata['source'].split(\"/\")[-1] for i in retrieved_docs]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6fffb721",
   "metadata": {},
   "source": [
    ":::{.callout-note}\n",
    "Note that we are using FAISS which defaults to Eucledian distance rather than cosine similarity, which we used in [previous](posts/2025-01-19-playing-with-embeddings/index.html) [experiements](/posts/2025-01-25-comparing-embedding-models/index.html). ChatGPT first suggested FAISS, but now it's telling me I should have used cosine similariy. ü§∑\n",
    ":::"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "e1a470c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4. For each retrieved document, use an LLM to summarize why it might be relevant.\n",
    "llm = OpenAI(temperature=0)  # low temperature for deterministic output\n",
    "\n",
    "def summarize_relevance(doc, query):\n",
    "    # Here we build a simple prompt to ask why the document is relevant to the query.\n",
    "    prompt = (\n",
    "        f\"Given the following text from a document:\\n\\n\"\n",
    "        f\"{doc.page_content}\\n\\n\"\n",
    "        f\"And the search query:\\n\\n\"\n",
    "        f\"{query}\\n\\n\"\n",
    "        f\"Summarize why this text is relevant to the query.\"\n",
    "    )\n",
    "    summary = llm.invoke(prompt)\n",
    "    return summary.strip()\n",
    "\n",
    "result = []\n",
    "for i, doc in enumerate(retrieved_docs):\n",
    "    relevance_summary = summarize_relevance(doc[0], query)\n",
    "    source_filename = doc[0].metadata['source'].split(\"/\")[-1]\n",
    "    excerpt = doc[0].page_content[:300] + \"...\"  # First 300 characters as excerpt\n",
    "    result.append({\n",
    "        \"relevance_summary\": relevance_summary,\n",
    "        \"source_filename\":source_filename,\n",
    "        \"excerpt\":excerpt,\n",
    "        \"score\":round(float(doc[1]),2)\n",
    "    })\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "808afd5f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "<table class=\"table plain\" border=\"1\" style=\"text-align:left; border-collapse: collapse; width: 100%; text-align: left;\">\n",
       "    <tr style=\"background-color: #f2f2f2;\">\n",
       "        <th>Relevance Summary</th>\n",
       "        <th>Excerpt</th>\n",
       "    </tr>\n",
       "\n",
       "    <tr style=\"background:white;\">\n",
       "    <td style=\"text-align:left;--bs-table-bg-type:none; font-weight:bold;\" colspan=\"2\">From Local to Global - A Graph RAG Approach to Query-Focused Summarization.pdf (score 0.37)</td>\n",
       "    </tr>\n",
       "    <tr style=\"background:white;\">\n",
       "        <td style=\"text-align:left; background-color:white;\">The text discusses the use of LLMs (Language Model Metrics) for evaluating natural language generation and conventional RAG (Retrieval-Augmented Generation) systems. It mentions that LLMs have been shown to be effective in measuring the qualities of generated texts and evaluating the performance of RAG systems. This is relevant to the query as it discusses the use of LLMs in searching and evaluating documents.</td>\n",
       "        <td style=\"text-align:left; font-style:italic\">\"relationship extraction only, with entity types and few-shot examples tailored to the domain of the\n",
       "data. The graph indexing process used a context window size of 600 tokens with 1 gleaning for the\n",
       "Podcast dataset and 0 gleanings for the News dataset.\n",
       "3.4 Metrics\n",
       "LLMs have been shown to be good eval...\"</td>\n",
       "    </tr>\n",
       "    \n",
       "    <tr style=\"background:white;\">\n",
       "    <td style=\"text-align:left;--bs-table-bg-type:none; font-weight:bold;\" colspan=\"2\">MemGPT - Towards LLMs as Operating Systems.pdf (score 0.39)</td>\n",
       "    </tr>\n",
       "    <tr style=\"background:white;\">\n",
       "        <td style=\"text-align:left; background-color:white;\">The text mentions using an LLM judge to evaluate whether an LLM correctly answered a question. This is relevant to the search query as it involves using LLMs to search documents. Additionally, the text discusses the format of the LLM response and the true answer, which could provide insight into how LLMs are used for document search.</td>\n",
       "        <td style=\"text-align:left; font-style:italic\">\"than from the model weights), we used an LLM judge. The\n",
       "LLM judge was provided the answers generated by both\n",
       "baseline approaches and MemGPT, and asked to make a\n",
       "judgement with the following prompt:\n",
       "Your task is to evaluate whether an LLM\n",
       "correct answered a question. The LLM\n",
       "response should be the fo...\"</td>\n",
       "    </tr>\n",
       "    \n",
       "    <tr style=\"background:white;\">\n",
       "    <td style=\"text-align:left;--bs-table-bg-type:none; font-weight:bold;\" colspan=\"2\">MemGPT - Towards LLMs as Operating Systems.pdf (score 0.4)</td>\n",
       "    </tr>\n",
       "    <tr style=\"background:white;\">\n",
       "        <td style=\"text-align:left; background-color:white;\">The text discusses the limitations of using LLMs (language model models) for conversational AI and their inability to handle long conversations or documents. It also mentions the challenges of extending the context length of transformers, which are used in LLMs, and the need for new long-context architectures. This is relevant to the query as it highlights the use of LLMs in searching documents and the challenges associated with it.</td>\n",
       "        <td style=\"text-align:left; font-style:italic\">\"have become the cornerstone of conversational AI and have\n",
       "led to a wide array of consumer and enterprise applications.\n",
       "Despite these advances, the limited fixed-length context\n",
       "windows used by LLMs significantly hinders their appli-\n",
       "cability to long conversations or reasoning about long doc-\n",
       "uments. ...\"</td>\n",
       "    </tr>\n",
       "    \n",
       "    <tr style=\"background:white;\">\n",
       "    <td style=\"text-align:left;--bs-table-bg-type:none; font-weight:bold;\" colspan=\"2\">MemGPT - Towards LLMs as Operating Systems.pdf (score 0.4)</td>\n",
       "    </tr>\n",
       "    <tr style=\"background:white;\">\n",
       "        <td style=\"text-align:left; background-color:white;\">ANSWER: The text discusses the use of an LLM judge to check the correctness of answers generated by baseline approaches and MemGPT for document analysis tasks. DOCUMENT: The text is relevant to the query because it specifically mentions the use of LLMs in searching documents, which is the main focus of the query. It also provides information on how the LLM judge was used to ensure that the answer was derived from the provided text rather than from model weights.</td>\n",
       "        <td style=\"text-align:left; font-style:italic\">\"response, provide both the answer\n",
       "and the document text from which you\n",
       "determined the answer. Format your\n",
       "response with the format ‚ÄôANSWER: <YOUR\n",
       "ANSWER>, DOCUMENT: [DOCUMENT TEXT]‚Äô. If\n",
       "none of the documents provided have\n",
       "the answer to the question, reply\n",
       "with ‚ÄôINSUFFICIENT INFORMATION‚Äô. Do\n",
       "NOT prov...\"</td>\n",
       "    </tr>\n",
       "    \n",
       "    <tr style=\"background:white;\">\n",
       "    <td style=\"text-align:left;--bs-table-bg-type:none; font-weight:bold;\" colspan=\"2\">Building effective agents _ Anthropic.pdf (score 0.4)</td>\n",
       "    </tr>\n",
       "    <tr style=\"background:white;\">\n",
       "        <td style=\"text-align:left; background-color:white;\">The text discusses the use of LLMs (language model models) in various workflows, particularly in cases where there are clear evaluation criteria and iterative refinement is beneficial. It also mentions specific examples where LLMs can be useful, such as in literary translation and complex search tasks. Additionally, it mentions the emergence of LLMs in production as their capabilities improve. This is relevant to the query as it discusses the use of LLMs in searching documents, which is the main focus of the query.</td>\n",
       "        <td style=\"text-align:left; font-style:italic\">\"The evaluator-optimizer work\u0000low\n",
       "When to use this workflow: This workflow is particularly effective\n",
       "when we have clear evaluation criteria, and when iterative refinement\n",
       "provides measurable value. The two signs of good fit are, first, that\n",
       "LLM responses can be demonstrably improved when a human\n",
       "arti...\"</td>\n",
       "    </tr>\n",
       "    \n",
       "    <tr style=\"background:white;\">\n",
       "    <td style=\"text-align:left;--bs-table-bg-type:none; font-weight:bold;\" colspan=\"2\">REACT - SYNERGIZING REASONING AND ACTING IN LANGUAGE MODELS.pdf (score 0.4)</td>\n",
       "    </tr>\n",
       "    <tr style=\"background:white;\">\n",
       "        <td style=\"text-align:left; background-color:white;\">The text discusses the use of LLMs (language models) for reasoning and problem solving, specifically mentioning the well-known work of Chain-of-Thought (CoT). It also mentions other follow-up works and studies that have been performed in this area. Additionally, it mentions the importance of symbols, patterns, and texts in the effectiveness of CoT. This text is relevant to the query because it discusses the use of LLMs in searching and analyzing documents.</td>\n",
       "        <td style=\"text-align:left; font-style:italic\">\"Published as a conference paper at ICLR 2023\n",
       "5 R ELATED WORK\n",
       "Language model for reasoning Perhaps the most well-known work of using LLMs for reasoning\n",
       "is Chain-of-Thought (CoT) (Wei et al., 2022), which reveals the ability of LLMs to formulate their\n",
       "own ‚Äúthinking procedure‚Äù for problem solving. Seve...\"</td>\n",
       "    </tr>\n",
       "    \n",
       "    <tr style=\"background:white;\">\n",
       "    <td style=\"text-align:left;--bs-table-bg-type:none; font-weight:bold;\" colspan=\"2\">RAGAS - Automated Evaluation of Retrieval Augmented Generation.pdf (score 0.41)</td>\n",
       "    </tr>\n",
       "    <tr style=\"background:white;\">\n",
       "        <td style=\"text-align:left; background-color:white;\">The text discusses the limitations of using Large Language Models (LLMs) as knowledge bases and the solution of Retrieval Augmented Generation (RAG) for answering questions. It also mentions the use of LLMs in searching for relevant passages in a corpus.</td>\n",
       "        <td style=\"text-align:left; font-style:italic\">\"knowledge emerged shortly after the introduction\n",
       "of BERT (Devlin et al., 2019) and became more\n",
       "firmly established with the introduction of ever\n",
       "larger LMs (Roberts et al., 2020). While the most\n",
       "recent Large Language Models (LLMs) capture\n",
       "enough knowledge to rival human performance\n",
       "across a wide vari...\"</td>\n",
       "    </tr>\n",
       "    \n",
       "    <tr style=\"background:white;\">\n",
       "    <td style=\"text-align:left;--bs-table-bg-type:none; font-weight:bold;\" colspan=\"2\">MemGPT - Towards LLMs as Operating Systems.pdf (score 0.41)</td>\n",
       "    </tr>\n",
       "    <tr style=\"background:white;\">\n",
       "        <td style=\"text-align:left; background-color:white;\">The text discusses the use of large language models (LLMs) in document analysis and multi-session chat, which are relevant to the query of using LLMs to search documents. It also mentions the limitations of LLMs' fixed-length context window and introduces a new OS-inspired design called MemGPT that aims to address this issue. The text also provides a link to access the code and data for experiments with MemGPT.</td>\n",
       "        <td style=\"text-align:left; font-style:italic\">\"to effectively provide extended context within\n",
       "the LLM‚Äôs limited context window. We evalu-\n",
       "ate our OS-inspired design in two domains where\n",
       "the limited context windows of modern LLMs\n",
       "severely handicaps their performance: document\n",
       "analysis, where MemGPT is able to analyze\n",
       "large documents that far exce...\"</td>\n",
       "    </tr>\n",
       "    \n",
       "    <tr style=\"background:white;\">\n",
       "    <td style=\"text-align:left;--bs-table-bg-type:none; font-weight:bold;\" colspan=\"2\">From Local to Global - A Graph RAG Approach to Query-Focused Summarization.pdf (score 0.41)</td>\n",
       "    </tr>\n",
       "    <tr style=\"background:white;\">\n",
       "        <td style=\"text-align:left; background-color:white;\">The text discusses the use of modern LLMs (language model machines) in various tasks, including summarization. It mentions specific LLMs such as GPT, Llama, and Gemini, which can use in-context learning to summarize content. However, the challenge remains for query-focused abstractive summarization over an entire corpus, as the volume of text can exceed the limits of LLM context windows. The text also mentions the possibility of using an alternative form of pre-indexing to support a new RAG (retrieval-augmented generation) approach for global summarization. This information is relevant to the query as it discusses the use of LLMs in searching and summarizing documents.</td>\n",
       "        <td style=\"text-align:left; font-style:italic\">\"and Lapata, 2019), these tasks are now trivialized by modern LLMs, including the GPT (Achiam\n",
       "et al., 2023; Brown et al., 2020), Llama (Touvron et al., 2023), and Gemini (Anil et al., 2023) series,\n",
       "all of which can use in-context learning to summarize any content provided in their context window.\n",
       "The...\"</td>\n",
       "    </tr>\n",
       "    \n",
       "    <tr style=\"background:white;\">\n",
       "    <td style=\"text-align:left;--bs-table-bg-type:none; font-weight:bold;\" colspan=\"2\">MemGPT - Towards LLMs as Operating Systems.pdf (score 0.41)</td>\n",
       "    </tr>\n",
       "    <tr style=\"background:white;\">\n",
       "        <td style=\"text-align:left; background-color:white;\">The text discusses the use of LLMs (large language models) in various applications, including using them as operating systems and for document retrieval. The references listed also mention the use of LLMs in improving language models and extending their context window. This is relevant to the query as it shows the potential use of LLMs in searching documents.</td>\n",
       "        <td style=\"text-align:left; font-style:italic\">\"MemGPT: Towards LLMs as Operating Systems\n",
       "References\n",
       "Iz Beltagy, Matthew E Peters, and Arman Cohan. Long-\n",
       "former: The long-document transformer. arXiv preprint\n",
       "arXiv:2004.05150, 2020.\n",
       "Sebastian Borgeaud, Arthur Mensch, Jordan Hoff-\n",
       "mann, Trevor Cai, Eliza Rutherford, Katie Millican,\n",
       "George Bm Van De...\"</td>\n",
       "    </tr>\n",
       "    </table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#| code-fold: true\n",
    "#| code-summary: \"Show the code\"\n",
    "\n",
    "html_output = \"\"\"\n",
    "<table class=\"table plain\" border=\"1\" style=\"text-align:left; border-collapse: collapse; width: 100%; text-align: left;\">\n",
    "    <tr style=\"background-color: #f2f2f2;\">\n",
    "        <th>Relevance Summary</th>\n",
    "        <th>Excerpt</th>\n",
    "    </tr>\n",
    "\"\"\"\n",
    "\n",
    "for i in result:\n",
    "    html_output += f\"\"\"\n",
    "    <tr style=\"background:white;\">\n",
    "    <td style=\"text-align:left;--bs-table-bg-type:none; font-weight:bold;\" colspan=\"2\">{i[\"source_filename\"]} (score {round(float(i[\"score\"]),2)})</td>\n",
    "    </tr>\n",
    "    <tr style=\"background:white;\">\n",
    "        <td style=\"text-align:left; background-color:white;\">{i[\"relevance_summary\"]}</td>\n",
    "        <td style=\"text-align:left; font-style:italic\">\"{i[\"excerpt\"]}\"</td>\n",
    "    </tr>\n",
    "    \"\"\"\n",
    "\n",
    "html_output += \"</table>\"\n",
    "display(HTML(html_output))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c72c55aa",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
